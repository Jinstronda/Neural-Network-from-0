16/10/2024
Creating a Neural Network from scratch to understand them better, for now it only works with Relu and with the last layer being linear, stil helped me understand Neural Networks in a much better way

Initialization:
n_neurons = Number of Neurons in The Layer
n_input = Number of Activations in the Previous Layer
type = Type of Activation Function

Type of Initilization Used for Neurons = He Inicialization


