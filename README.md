16/10/2024
Creating a Neural Network from scratch to understand them better, for now it only works with Relu and with the last layer being linear, stil helped me understand Neural Networks in a much better way

14/ 11 / 2024
Implemented ADAM solved all my issues, accuracy went from 92% to 98% with less epochs and without data augmentation. 
# Basic Neural Network Implementation
This repository contains a simple implementation of a feedforward neural network from scratch in Python, capable of training on datasets such as the handwritten digits dataset (MNIST). The code demonstrates basic principles of neural networks, including forward propagation, backward propagation (backpropagation), and gradient descent optimization.

# Features
Implements a neural network with customizable layers and activation functions.
Supports activation functions like ReLU and softmax.
Handles different types of loss functions, including Mean Squared Error (MSE) and Cross-Entropy Loss.
Implements backpropagation for training the network using gradient descent.
Supports training on any numerical dataset, such as the MNIST handwritten digits dataset.


